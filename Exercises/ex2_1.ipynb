{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2.1.1\n",
    "import importlib_resources\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "\n",
    "# Imports the numpy and xlrd package, then runs the ex2_1_1 code\n",
    "from ex2_1_1 import *\n",
    "from matplotlib.pyplot import figure, legend, plot, show, title, xlabel, ylabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xls sheet with data\n",
    "filename = importlib_resources.files(\"dtuimldmtools\").joinpath(\"data/nanonose.xls\")\n",
    "doc = xlrd.open_workbook(filename).sheet_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(filename, header=1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attribute names (1st row, column 4 to 12)\n",
    "attributeNames = doc.row_values(0, 3, 11)\n",
    "\n",
    "# Extract class names to python list,\n",
    "# then encode with integers (dict)\n",
    "classLabels = doc.col_values(0, 2, 92)\n",
    "classNames = sorted(set(classLabels))\n",
    "classDict = dict(zip(classNames, range(5)))\n",
    "\n",
    "# Extract vector y, convert to NumPy array\n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Preallocate memory, then extract excel data to matrix X\n",
    "X = np.empty((90, 8))\n",
    "for i, col_id in enumerate(range(3, 11)):\n",
    "    X[:, i] = np.asarray(doc.col_values(col_id, 2, 92))\n",
    "\n",
    "# Compute values of N, M and C.\n",
    "N = len(y)\n",
    "M = len(attributeNames)\n",
    "C = len(classNames)\n",
    "\n",
    "print(\"Ran Exercise 2.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data attributes to be plotted\n",
    "i = 2\n",
    "j = 4\n",
    "\n",
    "##\n",
    "# Make a simple plot of the i'th attribute against the j'th attribute\n",
    "# Notice that X is of matrix type (but it will also work with a numpy array)\n",
    "# X = np.array(X) #Try to uncomment this line\n",
    "plot(X[:, i], X[:, j], \"o\")\n",
    "\n",
    "# %%\n",
    "# Make another more fancy plot that includes legend, class labels,\n",
    "# attribute names, and a title.\n",
    "f = figure()\n",
    "title(\"NanoNose data\")\n",
    "\n",
    "for c in range(C):\n",
    "    # select indices belonging to class c:\n",
    "    class_mask = y == c\n",
    "    plot(X[class_mask, i], X[class_mask, j], \"o\", alpha=0.3)\n",
    "\n",
    "legend(classNames)\n",
    "xlabel(attributeNames[i])\n",
    "ylabel(attributeNames[j])\n",
    "\n",
    "# Output result to screen\n",
    "show()\n",
    "print(\"Ran Exercise 2.1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.3 Compute PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2.1.3\n",
    "# (requires data structures from ex. 2.2.1)\n",
    "import matplotlib.pyplot as plt\n",
    "from ex2_1_1 import *\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Subtract mean value from data\n",
    "Y = X - np.ones((N, 1)) * X.mean(axis=0)\n",
    "\n",
    "# PCA by computing SVD of Y\n",
    "U, S, V = svd(Y, full_matrices=False)\n",
    "\n",
    "# Compute variance explained by principal components\n",
    "rho = (S * S) / (S * S).sum()\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "# Plot variance explained\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(rho) + 1), rho, \"x-\")\n",
    "plt.plot(range(1, len(rho) + 1), np.cumsum(rho), \"o-\")\n",
    "plt.plot([1, len(rho)], [threshold, threshold], \"k--\")\n",
    "plt.title(\"Variance explained by principal components\")\n",
    "plt.xlabel(\"Principal component\")\n",
    "plt.ylabel(\"Variance explained\")\n",
    "plt.legend([\"Individual\", \"Cumulative\", \"Threshold\"])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ran Exercise 2.1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.4 Plot PCA1 vs PCA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2.1.4\n",
    "# (requires data structures from ex. 2.2.1 and 2.2.3)\n",
    "from ex2_1_1 import *\n",
    "from matplotlib.pyplot import figure, legend, plot, show, title, xlabel, ylabel\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Subtract mean value from data\n",
    "Y = X - np.ones((N, 1)) * X.mean(0)\n",
    "\n",
    "# PCA by computing SVD of Y\n",
    "U, S, Vh = svd(Y, full_matrices=False)\n",
    "# scipy.linalg.svd returns \"Vh\", which is the Hermitian (transpose)\n",
    "# of the vector V. So, for us to obtain the correct V, we transpose:\n",
    "V = Vh.T\n",
    "\n",
    "# Project the centered data onto principal component space\n",
    "Z = Y @ V\n",
    "\n",
    "# Indices of the principal components to be plotted\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "# Plot PCA of the data\n",
    "f = figure()\n",
    "title(\"NanoNose data: PCA\")\n",
    "# Z = array(Z)\n",
    "for c in range(C):\n",
    "    # select indices belonging to class c:\n",
    "    class_mask = y == c\n",
    "    plot(Z[class_mask, i], Z[class_mask, j], \"o\", alpha=0.5)\n",
    "legend(classNames)\n",
    "xlabel(\"PC{0}\".format(i + 1))\n",
    "ylabel(\"PC{0}\".format(j + 1))\n",
    "\n",
    "# Output result to screen\n",
    "show()\n",
    "\n",
    "print(\"Ran Exercise 2.1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.5 PCA Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2.2.4\n",
    "\n",
    "# (requires data structures from ex. 2.2.1)\n",
    "import matplotlib.pyplot as plt\n",
    "from ex2_1_1 import *\n",
    "from scipy.linalg import svd\n",
    "\n",
    "Y = X - np.ones((N, 1)) * X.mean(0)\n",
    "U, S, Vh = svd(Y, full_matrices=False)\n",
    "V = Vh.T\n",
    "N, M = X.shape\n",
    "\n",
    "# We saw in 2.1.3 that the first 3 components explaiend more than 90\n",
    "# percent of the variance. Let's look at their coefficients:\n",
    "pcs = [0, 1, 2]\n",
    "legendStrs = [\"PC\" + str(e + 1) for e in pcs]\n",
    "c = [\"r\", \"g\", \"b\"]\n",
    "bw = 0.2\n",
    "r = np.arange(1, M + 1)\n",
    "for i in pcs:\n",
    "    plt.bar(r + i * bw, V[:, i], width=bw)\n",
    "plt.xticks(r + bw, attributeNames)\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.ylabel(\"Component coefficients\")\n",
    "plt.legend(legendStrs)\n",
    "plt.grid()\n",
    "plt.title(\"NanoNose: PCA Component Coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# Inspecting the plot, we see that the 2nd principal component has large\n",
    "# (in magnitude) coefficients for attributes A, E and H. We can confirm\n",
    "# this by looking at it's numerical values directly, too:\n",
    "print(\"PC2:\")\n",
    "print(V[:, 1].T)\n",
    "\n",
    "# How does this translate to the actual data and its projections?\n",
    "# Looking at the data for water:\n",
    "\n",
    "# Projection of water class onto the 2nd principal component.\n",
    "all_water_data = Y[y == 4, :]\n",
    "\n",
    "print(\"First water observation\")\n",
    "print(all_water_data[0, :])\n",
    "\n",
    "# Based on the coefficients and the attribute values for the observation\n",
    "# displayed, would you expect the projection onto PC2 to be positive or\n",
    "# negative - why? Consider *both* the magnitude and sign of *both* the\n",
    "# coefficient and the attribute!\n",
    "\n",
    "# You can determine the projection by (remove comments):\n",
    "print(\"...and its projection onto PC2\")\n",
    "print(all_water_data[0, :] @ V[:, 1])\n",
    "# Try to explain why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.6 Investigate how standardization affects PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exercise 2.1.6\n",
    "import matplotlib.pyplot as plt\n",
    "from ex2_1_1 import *\n",
    "from scipy.linalg import svd\n",
    "\n",
    "r = np.arange(1, X.shape[1] + 1)\n",
    "plt.bar(r, np.std(X, 0))\n",
    "plt.xticks(r, attributeNames)\n",
    "plt.ylabel(\"Standard deviation\")\n",
    "plt.xlabel(\"Attributes\")\n",
    "plt.title(\"NanoNose: attribute standard deviations\")\n",
    "\n",
    "## Investigate how standardization affects PCA\n",
    "\n",
    "# Try this *later* (for last), and explain the effect\n",
    "# X_s = X.copy() # Make a to be \"scaled\" version of X\n",
    "# X_s[:, 2] = 100*X_s[:, 2] # Scale/multiply attribute C with a factor 100\n",
    "# Use X_s instead of X to in the script below to see the difference.\n",
    "# Does it affect the two columns in the plot equally?\n",
    "\n",
    "\n",
    "# Subtract the mean from the data\n",
    "Y1 = X - np.ones((N, 1)) * X.mean(0)\n",
    "\n",
    "# Subtract the mean from the data and divide by the attribute standard\n",
    "# deviation to obtain a standardized dataset:\n",
    "Y2 = X - np.ones((N, 1)) * X.mean(0)\n",
    "Y2 = Y2 * (1 / np.std(Y2, 0))\n",
    "# Here were utilizing the broadcasting of a row vector to fit the dimensions\n",
    "# of Y2\n",
    "\n",
    "# Store the two in a cell, so we can just loop over them:\n",
    "Ys = [Y1, Y2]\n",
    "titles = [\"Zero-mean\", \"Zero-mean and unit variance\"]\n",
    "threshold = 0.9\n",
    "# Choose two PCs to plot (the projection)\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "# Make the plot\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.title(\"NanoNose: Effect of standardization\")\n",
    "nrows = 3\n",
    "ncols = 2\n",
    "for k in range(2):\n",
    "    # Obtain the PCA solution by calculate the SVD of either Y1 or Y2\n",
    "    U, S, Vh = svd(Ys[k], full_matrices=False)\n",
    "    V = Vh.T  # For the direction of V to fit the convention in the course we transpose\n",
    "    # For visualization purposes, we flip the directionality of the\n",
    "    # principal directions such that the directions match for Y1 and Y2.\n",
    "    if k == 1:\n",
    "        V = -V\n",
    "        U = -U\n",
    "\n",
    "    # Compute variance explained\n",
    "    rho = (S * S) / (S * S).sum()\n",
    "\n",
    "    # Compute the projection onto the principal components\n",
    "    Z = U * S\n",
    "\n",
    "    # Plot projection\n",
    "    plt.subplot(nrows, ncols, 1 + k)\n",
    "    C = len(classNames)\n",
    "    for c in range(C):\n",
    "        plt.plot(Z[y == c, i], Z[y == c, j], \".\", alpha=0.5)\n",
    "    plt.xlabel(\"PC\" + str(i + 1))\n",
    "    plt.xlabel(\"PC\" + str(j + 1))\n",
    "    plt.title(titles[k] + \"\\n\" + \"Projection\")\n",
    "    plt.legend(classNames)\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    # Plot attribute coefficients in principal component space\n",
    "    plt.subplot(nrows, ncols, 3 + k)\n",
    "    for att in range(V.shape[1]):\n",
    "        plt.arrow(0, 0, V[att, i], V[att, j])\n",
    "        plt.text(V[att, i], V[att, j], attributeNames[att])\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.xlabel(\"PC\" + str(i + 1))\n",
    "    plt.ylabel(\"PC\" + str(j + 1))\n",
    "    plt.grid()\n",
    "    # Add a unit circle\n",
    "    plt.plot(\n",
    "        np.cos(np.arange(0, 2 * np.pi, 0.01)), np.sin(np.arange(0, 2 * np.pi, 0.01))\n",
    "    )\n",
    "    plt.title(titles[k] + \"\\n\" + \"Attribute coefficients\")\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    # Plot cumulative variance explained\n",
    "    plt.subplot(nrows, ncols, 5 + k)\n",
    "    plt.plot(range(1, len(rho) + 1), rho, \"x-\")\n",
    "    plt.plot(range(1, len(rho) + 1), np.cumsum(rho), \"o-\")\n",
    "    plt.plot([1, len(rho)], [threshold, threshold], \"k--\")\n",
    "    plt.title(\"Variance explained by principal components\")\n",
    "    plt.xlabel(\"Principal component\")\n",
    "    plt.ylabel(\"Variance explained\")\n",
    "    plt.legend([\"Individual\", \"Cumulative\", \"Threshold\"])\n",
    "    plt.grid()\n",
    "    plt.title(titles[k] + \"\\n\" + \"Variance explained\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
